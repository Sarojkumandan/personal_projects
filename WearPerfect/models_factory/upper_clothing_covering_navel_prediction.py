# -*- coding: utf-8 -*-
"""Upper_Clothing_Covering_Navel_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fBvoV4CabliJOPmBA1XK5YKV9N3G0j9X
"""

import os
import numpy as np
import pandas as pd
import cv2
import joblib
import seaborn as sns
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, Model, regularizers
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report, confusion_matrix, f1_score, balanced_accuracy_score
import keras_tuner as kt
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam

IMG_SIZE = (128, 128)
EPOCHS = 30
BASE_DIR = "/content/drive/MyDrive/upper_clothing_covering_navel_ensemble_tuning"
os.makedirs(BASE_DIR, exist_ok=True)

def load_data():
    # df = pd.read_csv("/content/drive/MyDrive/filtered_top_wear.csv")
    df = pd.read_csv("/content/drive/MyDrive/17k_csv/filtered_top_wear.csv")

    # df["Image_Path"] = "/content/drive/MyDrive/cropped_images/top_wear/" + df["Image_ID"]
    df["Image_Path"] = "/content/drive/MyDrive/cropped_images_17k/top_wear_17k/" + df["Image_ID"]

    df = df[df["upper_clothing_covering_navel"].isin(df["upper_clothing_covering_navel"].value_counts()[lambda x: x >= 5].index)]
    le = LabelEncoder()
    df["upper_clothing_covering_navel"] = le.fit_transform(df["upper_clothing_covering_navel"])
    joblib.dump(le, os.path.join(BASE_DIR, "navel_encoder.pkl"))
    return df[["Image_Path", "upper_clothing_covering_navel"]], le

def preprocess_image(path):
    try:
        img = cv2.imread(path)
        if img is None: return None
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = cv2.resize(img, IMG_SIZE)
        return img / 255.0
    except:
        return None

data, encoder = load_data()
labels = encoder.classes_
input_shape = (IMG_SIZE[0], IMG_SIZE[1], 3)

train_df, test_df = train_test_split(data, test_size=0.2, stratify=data["upper_clothing_covering_navel"], random_state=42)

X_train = np.array([preprocess_image(p) for p in train_df["Image_Path"] if preprocess_image(p) is not None])
y_train = train_df["upper_clothing_covering_navel"].iloc[:len(X_train)].values

X_test = np.array([preprocess_image(p) for p in test_df["Image_Path"] if preprocess_image(p) is not None])
y_test = test_df["upper_clothing_covering_navel"].iloc[:len(X_test)].values

class_weights = dict(zip(
    np.unique(y_train),
    compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
))


def build_transfer_model(hp, base_fn, input_shape, num_classes):
    base_model = base_fn(include_top=False, input_shape=input_shape, weights='imagenet')
    unfreeze_layers = hp.Choice('unfreeze_layers', [10, 20, 30])
    for layer in base_model.layers[:-unfreeze_layers]:
        layer.trainable = False
    for layer in base_model.layers[-unfreeze_layers:]:
        layer.trainable = True

    x = layers.GlobalAveragePooling2D()(base_model.output)
    x = layers.BatchNormalization()(x)
    x = layers.Dense(
        hp.Choice('dense_units', [64, 128, 256]),
        activation='relu',
        kernel_regularizer=regularizers.l2(hp.Choice('l2', [1e-3, 5e-4, 1e-4]))
    )(x)
    x = layers.Dropout(hp.Choice('dropout', [0.3, 0.4, 0.5]))(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)

    return Model(inputs=base_model.input, outputs=outputs)

def model_builder(hp):
    model = build_transfer_model(
        hp,
        tf.keras.applications.MobileNetV2,
        input_shape,
        len(labels)
    )
    model.compile(
        optimizer=Adam(learning_rate=hp.Choice('lr', [1e-4, 2e-5, 1e-5])),
        loss=tf.keras.losses.SparseCategoricalCrossentropy(),
        metrics=["sparse_categorical_accuracy"]
    )
    return model


tuner = kt.RandomSearch(
    model_builder,
    objective="val_sparse_categorical_accuracy",
    max_trials=10,
    directory=BASE_DIR,
    project_name="navel_tuning"
)

stop_early = EarlyStopping(monitor="val_loss", patience=3, restore_best_weights=True)

tuner.search(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=15,
    batch_size=32,
    class_weight=class_weights,
    callbacks=[stop_early],
    verbose=2
)

best_hps = tuner.get_best_hyperparameters(1)[0]
print("Best Hyperparameters:", best_hps.values)

base_models = {
    "mobilenet": tf.keras.applications.MobileNetV2,
    "resnet": tf.keras.applications.ResNet50,
    "efficientnet": tf.keras.applications.EfficientNetB0,
    "densenet": tf.keras.applications.DenseNet121
}

models_dict = {
    name: build_transfer_model(best_hps, base_fn, input_shape, len(labels))
    for name, base_fn in base_models.items()
}


# Store training history and validation losses
history_dict = {}
model_scores = {}

#  Extract best hyperparameters
lr = best_hps.get('lr') or best_hps.values.get('lr')
batch_size = 32  # You can tune this too if needed

best_val_loss = float('inf')
best_model = None
best_model_name = None

print(f" Using batch_size: {batch_size}, learning_rate: {lr}")

for name, model in models_dict.items():
    print(f"\n Training {name}")
    model.compile(
        optimizer=Adam(learning_rate=lr),
        loss=tf.keras.losses.SparseCategoricalCrossentropy(),
        metrics=["sparse_categorical_accuracy"]
    )

    history = model.fit(
        X_train, y_train,
        epochs=30,
        batch_size=batch_size,
        validation_data=(X_test, y_test),
        class_weight=class_weights,
        callbacks=[
            EarlyStopping(patience=5, restore_best_weights=True),
            ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=2, verbose=1)
        ],
        verbose=1
    )

    val_loss = min(history.history['val_loss'])
    model_scores[name] = val_loss
    history_dict[name] = history

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_model = model
        best_model_name = name

# Save only the best model
if best_model:
    print(f"\n Best Model: {best_model_name} (val_loss: {best_val_loss:.4f}) â€” Saving now!")
    best_model.save(os.path.join(BASE_DIR, f"best_model_{best_model_name}.keras"))

def ensemble_predict(models, X):
    preds = [model.predict(X, verbose=0) for model in models]
    return np.argmax(np.mean(preds, axis=0), axis=1)

def evaluate_model(title, y_true, y_pred, labels, y_train_true=None, y_train_pred=None):
    print(f"\n {title}")
    if y_train_true is not None:
        print("\n Training Evaluation:")
        print(classification_report(y_train_true, y_train_pred, target_names=labels, digits=4))
        print(f"Macro F1 Score: {f1_score(y_train_true, y_train_pred, average='macro'):.4f}")
        print(f"Balanced Accuracy: {balanced_accuracy_score(y_train_true, y_train_pred):.4f}")

    print("\n Testing Evaluation:")
    print(classification_report(y_true, y_pred, target_names=labels, digits=4))
    print(f"Macro F1 Score: {f1_score(y_true, y_pred, average='macro'):.4f}")
    print(f"Balanced Accuracy: {balanced_accuracy_score(y_true, y_pred):.4f}")

    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
    plt.title(f'{title} - Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.tight_layout()
    plt.show()

models_list = list(models_dict.values())
y_pred_test = ensemble_predict(models_list, X_test)
y_pred_train = ensemble_predict(models_list, X_train)

evaluate_model(
    "Ensemble - Upper Clothing Covering Navel (Final)",
    y_test, y_pred_test,
    encoder.classes_,
    y_train, y_pred_train
)