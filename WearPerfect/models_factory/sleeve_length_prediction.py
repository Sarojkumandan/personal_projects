# -*- coding: utf-8 -*-
"""Sleeve_length_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D2PpOkjOfpgFDI0V82-NmJnEp7ZHEEvt
"""

# !pip install -q keras-tuner opencv-python-headless

import os
import numpy as np
import pandas as pd
import cv2
import tensorflow as tf
import joblib
import keras_tuner as kt
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, f1_score, balanced_accuracy_score
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras import layers, Model, regularizers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau

IMG_SIZE = (128, 128)
EPOCHS = 20
BASE_DIR = "/content/drive/MyDrive/sleeve_length_ensemble_tuning"
os.makedirs(BASE_DIR, exist_ok=True)

def load_data():
    df = pd.read_csv("/content/drive/MyDrive/17k_csv/filtered_top_wear.csv")
    df["Image_Path"] = "/content/drive/MyDrive/cropped_images_17k/top_wear_17k/" + df["Image_ID"]
    df = df[df["sleeve_length"].isin(df["sleeve_length"].value_counts()[lambda x: x >= 5].index)]
    le = LabelEncoder()
    df["sleeve_length"] = le.fit_transform(df["sleeve_length"])
    joblib.dump(le, os.path.join(BASE_DIR, "sleeve_length_encoder.pkl"))
    return df[["Image_Path", "sleeve_length"]], le

def preprocess_image(path):
    try:
        img = cv2.imread(path)
        if img is None: return None
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = cv2.resize(img, IMG_SIZE)
        return img / 255.0
    except:
        return None

data, encoder = load_data()
labels = encoder.classes_
input_shape = (IMG_SIZE[0], IMG_SIZE[1], 3)

train_df, test_df = train_test_split(data, test_size=0.2, stratify=data["sleeve_length"], random_state=42)

X_train = np.array([preprocess_image(p) for p in train_df["Image_Path"] if preprocess_image(p) is not None])
y_train = train_df["sleeve_length"].iloc[:len(X_train)].values
X_test = np.array([preprocess_image(p) for p in test_df["Image_Path"] if preprocess_image(p) is not None])
y_test = test_df["sleeve_length"].iloc[:len(X_test)].values

class_weights = dict(zip(
    np.unique(y_train),
    compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
))

def build_single_model(base_fn, input_shape, hp, num_classes):
    base_model = base_fn(include_top=False, input_shape=input_shape, weights="imagenet")

    unfreeze_layers = hp.Choice("unfreeze_layers", [10, 20, 30])
    for layer in base_model.layers[:-unfreeze_layers]:
        layer.trainable = False
    for layer in base_model.layers[-unfreeze_layers:]:
        layer.trainable = True

    x = layers.GlobalAveragePooling2D()(base_model.output)
    x = layers.BatchNormalization()(x)
    x = layers.Dense(
        hp.Choice("dense_units", [64, 128, 256]),
        activation='relu',
        kernel_regularizer=regularizers.l2(hp.Choice("l2_reg", [1e-3, 5e-4, 1e-4]))
    )(x)
    x = layers.Dropout(hp.Choice("dropout", [0.3, 0.4, 0.5]))(x)
    output = layers.Dense(num_classes, activation="softmax")(x)

    return Model(inputs=base_model.input, outputs=output)

def build_ensemble_model(hp):
    input_tensor = layers.Input(shape=input_shape)

    models = [
        build_single_model(tf.keras.applications.MobileNetV2, input_shape, hp, len(labels)),
        build_single_model(tf.keras.applications.ResNet50, input_shape, hp, len(labels)),
        build_single_model(tf.keras.applications.EfficientNetB0, input_shape, hp, len(labels)),
        build_single_model(tf.keras.applications.DenseNet121, input_shape, hp, len(labels))
    ]

    outputs = [model(input_tensor) for model in models]
    avg_output = layers.Average()(outputs)

    model = Model(inputs=input_tensor, outputs=avg_output)

    hp_learning_rate = hp.Choice("learning_rate", [1e-4, 2e-5, 1e-5])

    model.compile(
        optimizer=Adam(learning_rate=hp_learning_rate),
        loss=tf.keras.losses.SparseCategoricalCrossentropy(),
        metrics=["sparse_categorical_accuracy"]
    )
    return model

tuner = kt.RandomSearch(
    build_ensemble_model,
    objective="val_sparse_categorical_accuracy",
    max_trials=5,
    directory=BASE_DIR,
    project_name="ensemble_tuning"
)

tuner.search_space_summary()

tuner.search(
    X_train, y_train,
    epochs=15,
    validation_data=(X_test, y_test),
    class_weight=class_weights,
    callbacks=[
        EarlyStopping(patience=4, restore_best_weights=True),
        ReduceLROnPlateau(patience=2)
    ],
    batch_size=32
)

best_model = tuner.get_best_models(num_models=1)[0]

y_pred_test = np.argmax(best_model.predict(X_test), axis=1)
y_pred_train = np.argmax(best_model.predict(X_train), axis=1)

print("\n Training Results")
print(classification_report(y_train, y_pred_train, target_names=labels))
print(f"Macro F1: {f1_score(y_train, y_pred_train, average='macro'):.4f}")

print("\n Testing Results")
print(classification_report(y_test, y_pred_test, target_names=labels))
print(f"Macro F1: {f1_score(y_test, y_pred_test, average='macro'):.4f}")