# -*- coding: utf-8 -*-
"""Outer_Clothing_Cardigan_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/108byHRS5pvC67m5uWVUk9vmYdFEOtBdM
"""

# !pip install -q keras-tuner opencv-python-headless

import os
import numpy as np
import pandas as pd
import cv2
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
from tqdm import tqdm
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report, confusion_matrix, f1_score, balanced_accuracy_score
import tensorflow as tf
from tensorflow.keras import layers, regularizers, Model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
import keras_tuner as kt
from sklearn.metrics import classification_report, confusion_matrix, f1_score, balanced_accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

# Cell 2: Config
IMG_SIZE = (128, 128)
BATCH_SIZE = 32
EPOCHS = 30
BASE_DIR = "/content/drive/MyDrive/outer_cardigan_ensemble_tuning"
os.makedirs(BASE_DIR, exist_ok=True)

# Cell 3: Load Data and Encode

def load_data():
    # df = pd.read_csv("/content/drive/MyDrive/filtered_top_wear.csv")
    df = pd.read_csv("/content/drive/MyDrive/17k_csv/filtered_top_wear.csv")

    df = df[df["outer_clothing_cardigan"].notna()]
    # df["Image_Path"] = "/content/drive/MyDrive/cropped_images/top_wear/" + df["Image_ID"]
    df["Image_Path"] = "/content/drive/MyDrive/cropped_images_17k/top_wear_17k/" + df["Image_ID"]

    df = df[df["outer_clothing_cardigan"].isin(df["outer_clothing_cardigan"].value_counts()[lambda x: x >= 5].index)]
    le = LabelEncoder()
    df["outer_clothing_cardigan"] = le.fit_transform(df["outer_clothing_cardigan"])
    joblib.dump(le, os.path.join(BASE_DIR, "outer_cardigan_encoder.pkl"))
    return df[["Image_Path", "outer_clothing_cardigan"]], le

# Cell 4: Preprocessing

def preprocess_image(path):
    try:
        img = cv2.imread(path)
        if img is None: return None
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = cv2.resize(img, IMG_SIZE)
        return img / 255.0
    except:
        return None

# Cell 5: Load + Split Data
data, encoder = load_data()
labels = encoder.classes_
input_shape = (IMG_SIZE[0], IMG_SIZE[1], 3)

train_df, test_df = train_test_split(data, test_size=0.2, stratify=data["outer_clothing_cardigan"], random_state=42)

X_train = np.array([preprocess_image(p) for p in train_df["Image_Path"] if preprocess_image(p) is not None])
y_train = train_df["outer_clothing_cardigan"].iloc[:len(X_train)].values
X_test = np.array([preprocess_image(p) for p in test_df["Image_Path"] if preprocess_image(p) is not None])
y_test = test_df["outer_clothing_cardigan"].iloc[:len(X_test)].values

# Compute Class Weights
class_weights = dict(zip(
    np.unique(y_train),
    compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)
))

# Cell 6: Build Transfer Model

def build_transfer_model(hp, base_fn, input_shape, num_classes):
    base_model = base_fn(include_top=False, input_shape=input_shape, weights='imagenet')
    unfrozen_layers = hp.Choice('unfreeze_layers', [10, 20, 30])
    for layer in base_model.layers[:-unfrozen_layers]:
        layer.trainable = False
    for layer in base_model.layers[-unfrozen_layers:]:
        layer.trainable = True

    x = layers.GlobalAveragePooling2D()(base_model.output)
    x = layers.BatchNormalization()(x)
    x = layers.Dense(
        hp.Choice('dense_units', [64, 128, 256]),
        activation='relu',
        kernel_regularizer=regularizers.l2(hp.Choice('l2', [1e-3, 5e-4, 1e-4])))(x)
    x = layers.Dropout(hp.Choice('dropout', [0.3, 0.4, 0.5]))(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)

    return Model(inputs=base_model.input, outputs=outputs)

# Cell 7: Tuner Class

def model_builder(hp):
    model = build_transfer_model(hp, tf.keras.applications.MobileNetV2, input_shape, len(labels))
    model.compile(
        optimizer=Adam(learning_rate=hp.Choice('lr', [1e-4, 2e-5, 1e-5])),
        loss=tf.keras.losses.SparseCategoricalCrossentropy(),
        metrics=['sparse_categorical_accuracy']
    )
    return model

# Cell 8: Keras Tuner Run

tuner = kt.RandomSearch(
    model_builder,
    objective='val_sparse_categorical_accuracy',
    max_trials=10,
    directory=BASE_DIR,
    project_name='outer_cardigan_tuning'
)

tuner.search(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=15,
    batch_size=32,
    class_weight=class_weights,
    callbacks=[EarlyStopping(patience=3, restore_best_weights=True)],
    verbose=2
)

# Cell 9: Final Ensemble Models with Best Params
best_hps = tuner.get_best_hyperparameters(1)[0]
print(best_hps.values)
base_models = {
    "mobilenet": tf.keras.applications.MobileNetV2,
    "resnet": tf.keras.applications.ResNet50,
    "efficientnet": tf.keras.applications.EfficientNetB0,
    "densenet": tf.keras.applications.DenseNet121
}

# Pass `best_hps` as the first argument
models_dict = {
    name: build_transfer_model(best_hps, base_fn, input_shape, len(labels))
    for name, base_fn in base_models.items()
}

history_dict = {}
model_scores = {}

# Extract common hyperparameters
batch_size = 32
learning_rate = best_hps.values.get('lr')

print(f"Batch Size: {batch_size}, Learning Rate: {learning_rate}")

best_val_loss = float("inf")
best_model_name = None
best_model = None

for name, model in models_dict.items():
    print(f"\n Training {name}")
    model.compile(
        optimizer=Adam(learning_rate=learning_rate),
        loss=tf.keras.losses.SparseCategoricalCrossentropy(),
        metrics=["sparse_categorical_accuracy"]
    )

    history = model.fit(
        X_train, y_train,
        epochs=30,
        batch_size=batch_size,
        validation_data=(X_test, y_test),
        class_weight=class_weights,
        callbacks=[
            EarlyStopping(patience=5, restore_best_weights=True),
            ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=2, verbose=1)
        ],
        verbose=1
    )

    val_loss = min(history.history["val_loss"])  # Get lowest val_loss
    model_scores[name] = val_loss
    history_dict[name] = history

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_model_name = name
        best_model = model

#  Save only the best model across all four
if best_model:
    print(f"\n Saving best model: {best_model_name} with val_loss: {best_val_loss:.4f}")
    best_model.save(os.path.join(BASE_DIR, f"outer_cardigan_best_model_{best_model_name}.keras"))


#  Ensemble prediction via soft voting
def ensemble_predict(models, X):
    preds = [model.predict(X, verbose=0) for model in models]
    return np.argmax(np.mean(preds, axis=0), axis=1)

# Evaluation function for both train & test
def evaluate_model(title, y_true, y_pred, labels, y_train_true=None, y_train_pred=None):
    print(f"\n {title}")

    # Training Evaluation
    if y_train_true is not None and y_train_pred is not None:
        print("\n Training Evaluation:")
        print(classification_report(y_train_true, y_train_pred, target_names=labels, digits=4))
        print(f"Macro F1 Score: {f1_score(y_train_true, y_train_pred, average='macro'):.4f}")
        print(f"Balanced Accuracy: {balanced_accuracy_score(y_train_true, y_train_pred):.4f}")

    # Testing Evaluation
    print("\n Testing Evaluation:")
    print(classification_report(y_true, y_pred, target_names=labels, digits=4))
    print(f"Macro F1 Score: {f1_score(y_true, y_pred, average='macro'):.4f}")
    print(f"Balanced Accuracy: {balanced_accuracy_score(y_true, y_pred):.4f}")

    # Confusion Matrix
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
    plt.title(f'{title} - Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.tight_layout()
    plt.show()

# Get model predictions
models_list = list(models_dict.values())
y_pred_test = ensemble_predict(models_list, X_test)
y_pred_train = ensemble_predict(models_list, X_train)

# Evaluate
evaluate_model(
    "Ensemble - Outer Clothing Cardigan (Final Evaluation)",
    y_test, y_pred_test,
    ["no cardigan", "yes cardigan"],
    y_train, y_pred_train
)